#!/usr/bin/env python
"""
Optimize CustomKB performance for knowledge bases in VECTORDBS.
This script applies performance optimizations to existing KB configurations.
"""

import os
import sys
import shutil
import configparser
import argparse
from datetime import datetime
from pathlib import Path

# Add parent directory to path for imports
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from config.config_manager import get_fq_cfg_filename

def get_system_memory_gb():
    """Get total system memory in GB."""
    try:
        import psutil
        return psutil.virtual_memory().total / (1024**3)
    except:
        # Fallback to reading /proc/meminfo
        try:
            with open('/proc/meminfo', 'r') as f:
                for line in f:
                    if line.startswith('MemTotal:'):
                        kb = int(line.split()[1])
                        return kb / (1024**2)
        except:
            return 16  # Safe default

def get_optimized_settings(memory_gb: float = None):
    """
    Get optimized settings based on available system memory.
    
    Memory tiers:
    - Low: < 16GB (conservative settings)
    - Medium: 16-64GB (balanced performance)
    - High: 64-128GB (high performance)
    - Very High: > 128GB (maximum performance)
    """
    if memory_gb is None:
        memory_gb = get_system_memory_gb()
    
    # Base settings that scale with memory
    if memory_gb < 16:
        # Low memory settings (conservative)
        tier = "low"
        memory_factor = 0.25
        thread_factor = 0.5
        batch_factor = 0.5
    elif memory_gb < 64:
        # Medium memory settings (balanced)
        tier = "medium"
        memory_factor = 0.5
        thread_factor = 0.75
        batch_factor = 0.75
    elif memory_gb < 128:
        # High memory settings (performance)
        tier = "high"
        memory_factor = 0.75
        thread_factor = 1.0
        batch_factor = 1.0
    else:
        # Very high memory settings (maximum performance)
        tier = "very_high"
        memory_factor = 1.0
        thread_factor = 1.5
        batch_factor = 1.5
    
    # Calculate scaled values
    memory_cache = int(500000 * memory_factor)
    io_threads = max(4, int(32 * thread_factor))
    cache_threads = max(4, int(16 * thread_factor))
    api_concurrency = max(8, int(32 * thread_factor))
    api_min_concurrency = max(3, int(16 * thread_factor))
    embedding_batch = int(1000 * batch_factor)
    file_batch = int(5000 * batch_factor)
    sql_batch = int(5000 * batch_factor)
    reference_batch = max(5, int(50 * batch_factor))
    reranking_batch = int(64 * batch_factor)
    reranking_cache = int(10000 * memory_factor)
    
    return {
        'tier': tier,
        'memory_gb': memory_gb,
        'optimizations': {
            'DEFAULT': {
                'query_top_k': '30',
            },
            'API': {
                'api_call_delay_seconds': '0.01',
                'api_max_concurrency': str(api_concurrency),
                'api_min_concurrency': str(api_min_concurrency),
            },
            'LIMITS': {
                'max_file_size_mb': str(int(500 * batch_factor)),
                'max_query_file_size_mb': str(max(1, int(10 * batch_factor))),
                'memory_cache_size': str(memory_cache),
                'max_query_length': str(int(50000 * batch_factor)),
                'max_config_value_length': str(int(5000 * batch_factor)),
                'max_json_size': str(int(50000 * batch_factor)),
            },
            'PERFORMANCE': {
                'embedding_batch_size': str(embedding_batch),
                'checkpoint_interval': str(max(10, int(50 * batch_factor))),
                'commit_frequency': str(int(5000 * batch_factor)),
                'io_thread_pool_size': str(io_threads),
                'cache_thread_pool_size': str(cache_threads),
                'file_processing_batch_size': str(file_batch),
                'sql_batch_size': str(sql_batch),
                'reference_batch_size': str(reference_batch),
                'query_cache_ttl_days': '30',
            },
            'ALGORITHMS': {
                'small_dataset_threshold': str(int(10000 * batch_factor)),
                'medium_dataset_threshold': str(int(1000000 * batch_factor)),
                'ivf_centroid_multiplier': str(max(4, int(8 * batch_factor))),
                'max_centroids': str(int(1024 * batch_factor)),
                'token_estimation_sample_size': str(int(50 * batch_factor)),
                'max_chunk_overlap': str(int(200 * batch_factor)),
                'heading_search_limit': str(int(500 * batch_factor)),
                'entity_extraction_limit': str(int(1000 * batch_factor)),
                'enable_hybrid_search': 'true' if memory_gb >= 16 else 'false',
                'bm25_rebuild_threshold': str(int(5000 * batch_factor)),
                'bm25_max_results': str(int(1000 * memory_factor)),  # Limit BM25 results
                'max_synonyms_per_word': str(max(2, int(3 * memory_factor))),
                'query_enhancement_cache_ttl_days': '60',
                'reranking_top_k': str(int(50 * batch_factor)),
                'reranking_batch_size': str(reranking_batch),
                'reranking_device': 'cuda:0' if memory_gb >= 32 else 'cpu',
                'reranking_cache_size': str(reranking_cache),
                # GPU optimization for FAISS
                'faiss_gpu_batch_size': str(int(2048 * batch_factor)) if memory_gb >= 32 else '1024',
                'faiss_gpu_use_float16': 'true' if memory_gb >= 32 else 'false',
            }
        }
    }

def find_kb_configs(vectordbs_dir: str) -> list:
    """Find all knowledge base configuration files in VECTORDBS."""
    configs = []
    
    for root, dirs, files in os.walk(vectordbs_dir):
        # Skip hidden directories and virtual environments
        dirs[:] = [d for d in dirs if not d.startswith('.') and d not in ['.venv', 'venv', '__pycache__']]
        
        for file in files:
            if file.endswith('.cfg'):
                config_path = os.path.join(root, file)
                # Skip backup files and pyvenv.cfg files
                if ('backup' not in config_path and 
                    'optimized' not in config_path and
                    file != 'pyvenv.cfg'):
                    configs.append(config_path)
    
    return sorted(configs)

def backup_config(config_path: str) -> str:
    """Create a backup of the configuration file."""
    import logging
    logger = logging.getLogger(__name__)
    
    backup_dir = os.path.join(os.path.dirname(config_path), 'backups')
    os.makedirs(backup_dir, exist_ok=True)
    
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    backup_name = f"{os.path.basename(config_path)}.{timestamp}.bak"
    backup_path = os.path.join(backup_dir, backup_name)
    
    shutil.copy2(config_path, backup_path)
    logger.info(f"Created backup: {backup_path}")
    
    return backup_path

def optimize_config(config_path: str, dry_run: bool = False, memory_gb: float = None) -> dict:
    """Apply performance optimizations to a configuration file."""
    import logging
    logger = logging.getLogger(__name__)
    logger.info(f"Processing: {config_path}")
    
    # Check FAISS index size
    kb_dir = os.path.dirname(config_path)
    kb_name = os.path.basename(kb_dir)
    faiss_file = os.path.join(kb_dir, f"{kb_name}.faiss")
    faiss_size_mb = 0
    if os.path.exists(faiss_file):
        faiss_size_mb = os.path.getsize(faiss_file) / (1024 * 1024)
    
    # Get optimized settings based on system memory
    settings_info = get_optimized_settings(memory_gb)
    performance_optimizations = settings_info['optimizations']
    
    # Adjust GPU settings based on FAISS index size
    # GPU memory limit: ~15GB for FAISS index on 23GB GPU
    if faiss_size_mb > 15000:  # 15GB limit
        logger.info(f"FAISS index too large for GPU ({faiss_size_mb:.1f} MB), using CPU settings")
        performance_optimizations['ALGORITHMS']['reranking_device'] = 'cuda:0'  # Still use GPU for reranking
        performance_optimizations['ALGORITHMS']['faiss_gpu_batch_size'] = '2048'
        performance_optimizations['ALGORITHMS']['faiss_gpu_use_float16'] = 'true'
        # Note: The actual GPU loading will be handled by query_manager.py
    
    # Load existing configuration
    config = configparser.ConfigParser()
    config.read(config_path)
    
    # Track changes
    changes = {}
    
    # Apply optimizations
    for section, settings in performance_optimizations.items():
        if section not in config:
            if section != 'DEFAULT':
                config.add_section(section)
        
        for key, new_value in settings.items():
            old_value = config.get(section, key, fallback=None)
            
            # Only update if different
            if old_value != new_value:
                if not dry_run:
                    config.set(section, key, new_value)
                
                if section not in changes:
                    changes[section] = {}
                changes[section][key] = {
                    'old': old_value,
                    'new': new_value
                }
    
    # Save optimized configuration
    if not dry_run and changes:
        # Create backup first
        backup_path = backup_config(config_path)
        
        # Write optimized config
        with open(config_path, 'w') as f:
            config.write(f)
        
        logger.info(f"Updated configuration: {config_path}")
        logger.info(f"Backup saved to: {backup_path}")
    
    return changes

def print_changes(config_path: str, changes: dict):
    """Print configuration changes in a readable format."""
    if not changes:
        print(f"\n{config_path}: No changes needed (already optimized)")
        return
    
    print(f"\n{config_path}:")
    print("-" * 80)
    
    for section, settings in changes.items():
        print(f"\n[{section}]")
        for key, values in settings.items():
            old = values['old'] or '<not set>'
            new = values['new']
            print(f"  {key}: {old} -> {new}")

def analyze_kb_size(config_path: str) -> dict:
    """Analyze the size of a knowledge base."""
    kb_dir = os.path.dirname(config_path)
    stats = {
        'config_path': config_path,
        'kb_name': os.path.basename(kb_dir),
        'db_size_mb': 0,
        'vector_size_mb': 0,
        'total_size_mb': 0
    }
    
    # Check for database file
    db_files = [f for f in os.listdir(kb_dir) if f.endswith('.db')]
    if db_files:
        db_path = os.path.join(kb_dir, db_files[0])
        if os.path.exists(db_path):
            stats['db_size_mb'] = os.path.getsize(db_path) / 1024 / 1024
    
    # Check for vector file
    vector_files = [f for f in os.listdir(kb_dir) if f.endswith('.faiss')]
    if vector_files:
        vector_path = os.path.join(kb_dir, vector_files[0])
        if os.path.exists(vector_path):
            stats['vector_size_mb'] = os.path.getsize(vector_path) / 1024 / 1024
    
    stats['total_size_mb'] = stats['db_size_mb'] + stats['vector_size_mb']
    
    return stats

def main():
    """Main entry point."""
    parser = argparse.ArgumentParser(
        description='Optimize CustomKB performance for production use'
    )
    parser.add_argument(
        'target',
        nargs='?',
        help='Specific KB config file or directory to optimize (default: all KBs in VECTORDBS)'
    )
    parser.add_argument(
        '--dry-run',
        action='store_true',
        help='Show what would be changed without modifying files'
    )
    parser.add_argument(
        '--analyze',
        action='store_true',
        help='Analyze KB sizes and show optimization recommendations'
    )
    parser.add_argument(
        '--vectordbs',
        default=os.getenv('VECTORDBS', '/var/lib/vectordbs'),
        help='Override VECTORDBS directory'
    )
    parser.add_argument(
        '--memory-gb',
        type=float,
        help='Override system memory detection (in GB)'
    )
    
    args = parser.parse_args()
    
    # Setup simple console logging for this utility
    import logging
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )
    logger = logging.getLogger(__name__)
    
    # Find configurations to process
    if args.target:
        if os.path.isfile(args.target) and args.target.endswith('.cfg'):
            configs = [args.target]
        elif os.path.isdir(args.target):
            configs = find_kb_configs(args.target)
        else:
            # Try to find it as a KB name in VECTORDBS
            potential_path = get_fq_cfg_filename(args.target)
            if potential_path:
                configs = [potential_path]
            else:
                logger.error(f"Cannot find configuration: {args.target}")
                return 1
    else:
        configs = find_kb_configs(args.vectordbs)
    
    if not configs:
        logger.warning(f"No knowledge base configurations found in {args.vectordbs}")
        return 0
    
    print(f"\nFound {len(configs)} knowledge base configuration(s)")
    
    # Show system memory info
    memory_gb = args.memory_gb or get_system_memory_gb()
    settings_info = get_optimized_settings(memory_gb)
    
    print(f"\nSystem Memory: {memory_gb:.1f} GB")
    print(f"Optimization Tier: {settings_info['tier'].upper()}")
    print(f"  - Memory cache size: {settings_info['optimizations']['LIMITS']['memory_cache_size']}")
    print(f"  - Reference batch size: {settings_info['optimizations']['PERFORMANCE']['reference_batch_size']}")
    print(f"  - Thread pools: {settings_info['optimizations']['PERFORMANCE']['io_thread_pool_size']}")
    
    if args.analyze:
        # Analyze mode - show KB sizes and recommendations
        total_size = 0
        kb_stats = []
        
        for config in configs:
            stats = analyze_kb_size(config)
            kb_stats.append(stats)
            total_size += stats['total_size_mb']
        
        # Sort by size
        kb_stats.sort(key=lambda x: x['total_size_mb'], reverse=True)
        
        print("\nKnowledge Base Analysis:")
        print("-" * 80)
        print(f"{'KB Name':<30} {'DB (MB)':<10} {'Vector (MB)':<12} {'Total (MB)':<10}")
        print("-" * 80)
        
        for stats in kb_stats:
            print(f"{stats['kb_name']:<30} "
                  f"{stats['db_size_mb']:<10.1f} "
                  f"{stats['vector_size_mb']:<12.1f} "
                  f"{stats['total_size_mb']:<10.1f}")
        
        print("-" * 80)
        print(f"{'Total:':<30} {'':<10} {'':<12} {total_size:<10.1f}")
        
        print("\nOptimization Recommendations:")
        print("- Large KBs (>1GB) will benefit most from increased batch sizes")
        print("- KBs with many queries benefit from larger memory caches")
        
        # Tier-specific recommendations
        if settings_info['tier'] == 'low':
            print("\nLow Memory Tier (<16GB):")
            print("- Conservative settings to avoid memory pressure")
            print("- Consider upgrading RAM for better performance")
        elif settings_info['tier'] == 'medium':
            print("\nMedium Memory Tier (16-64GB):")
            print("- Balanced settings for good performance")
            print("- Suitable for most workloads")
        elif settings_info['tier'] == 'high':
            print("\nHigh Memory Tier (64-128GB):")
            print("- High performance settings")
            print("- Excellent for production workloads")
        else:
            print("\nVery High Memory Tier (>128GB):")
            print("- Maximum performance settings")
            print("- Optimal for large-scale deployments")
        
    else:
        # Optimization mode
        if args.dry_run:
            print("\nDRY RUN MODE - No changes will be made")
        
        all_changes = {}
        
        for config in configs:
            changes = optimize_config(config, dry_run=args.dry_run, memory_gb=memory_gb)
            if changes:
                all_changes[config] = changes
            print_changes(config, changes)
        
        if all_changes:
            print(f"\n{'Would optimize' if args.dry_run else 'Optimized'} "
                  f"{len(all_changes)} configuration(s)")
            
            if args.dry_run:
                print("\nTo apply these optimizations, run without --dry-run")
        else:
            print("\nAll configurations are already optimized!")
    
    return 0

if __name__ == '__main__':
    sys.exit(main())

#fin