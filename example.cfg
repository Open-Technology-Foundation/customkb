# Example CustomKB Configuration File
# 
# This file demonstrates all available configuration options with recommended values.
# Copy this file and modify for your specific use case.
#
# Configuration hierarchy:
# 1. Environment variables (highest priority)
# 2. Values in this file
# 3. Built-in defaults (lowest priority)

[DEFAULT]
# Core embedding and database settings

# Embedding model for vector generation
# Options: text-embedding-ada-002, text-embedding-3-small, text-embedding-3-large, gemini-embedding-001
vector_model = text-embedding-3-small

# Embedding dimensions (must match model output)
# ada-002: 1536, 3-small: 1536, 3-large: 3072
# gemini-embedding-001: configurable (768, 1536, or 3072)
vector_dimensions = 1536

# Number of chunks to process per embedding batch
vector_chunks = 500

# Token limits for text chunking
# Smaller chunks = more precise but more API calls
# Larger chunks = fewer calls but less granular search
db_min_tokens = 200
db_max_tokens = 400

# Query response settings
# Available models: gpt-4o, gpt-4o-mini, claude-3-5-sonnet, claude-3-5-haiku
query_model = gpt-4o-mini
query_max_tokens = 4096
query_top_k = 50                    # Number of search results to retrieve
query_context_scope = 4             # Context segments around each result
query_temperature = 0.1             # Lower = more focused, higher = more creative
query_role = You are a helpful assistant. Answer questions accurately based on the provided context.

# Additional context files (comma-separated)
# These files are always included in query context
query_context_files = 

[API]
# External API configuration

# Rate limiting (seconds between API calls)
api_call_delay_seconds = 0.05

# Retry configuration
api_max_retries = 20
backoff_exponent = 2
backoff_jitter = 0.1

# Concurrency settings
api_max_concurrency = 8             # Maximum parallel API requests
api_min_concurrency = 3             # Minimum parallel requests

[LIMITS]
# Resource and security limits

# File size limits
max_file_size_mb = 100              # Maximum input file size
max_query_file_size_mb = 1          # Maximum query file size

# Memory and caching
memory_cache_size = 10000           # Number of embeddings to cache in memory

# Security constraints
api_key_min_length = 20             # Minimum API key length
max_query_length = 10000            # Maximum query text length
max_config_value_length = 1000      # Maximum config parameter length
max_json_size = 10000               # Maximum JSON response size

[PERFORMANCE]
# Performance tuning parameters

# Batch processing
embedding_batch_size = 100          # Embeddings per batch
file_processing_batch_size = 500    # Files per batch
sql_batch_size = 500                # Database operations per batch
reference_batch_size = 5            # References per query batch

# Checkpointing and persistence
checkpoint_interval = 10            # Save progress every N batches
commit_frequency = 1000             # Database commits per N operations

# Threading and caching
io_thread_pool_size = 4             # Parallel I/O threads
query_cache_ttl_days = 7            # Query cache expiration

# Editor preference
default_editor = nano               # Editor for 'customkb edit' command

[ALGORITHMS]
# Algorithm-specific parameters

# Index selection thresholds
high_dimension_threshold = 1536     # Use specialized index above this
small_dataset_threshold = 1000      # Use flat index below this
medium_dataset_threshold = 100000   # Use IVF index below this

# FAISS index parameters
ivf_centroid_multiplier = 4         # Centroids = sqrt(vectors) * multiplier
max_centroids = 256                 # Maximum IVF centroids

# Text processing
token_estimation_sample_size = 10   # Samples for token counting
token_estimation_multiplier = 1.3   # Safety factor for estimates
max_chunk_overlap = 100             # Token overlap between chunks
overlap_ratio = 0.5                 # Overlap as fraction of chunk size

# Search parameters
similarity_threshold = 0.6          # Minimum similarity score
low_similarity_scope_factor = 0.5   # Scope reduction for poor matches

# Metadata extraction
heading_search_limit = 200          # Characters to scan for headings
entity_extraction_limit = 500       # Maximum entities to extract

# System settings
default_dir_permissions = 0o770     # Directory creation permissions
default_code_language = python      # Default for code detection

# Additional languages for stopword filtering
# Available: arabic, azerbaijani, basque, bengali, catalan, chinese, danish,
# dutch, english, finnish, french, german, greek, hebrew, hungarian, indonesian,
# italian, kazakh, nepali, norwegian, portuguese, romanian, russian, slovene,
# spanish, swedish, tajik, turkish
additional_stopword_languages = french,german,spanish

# BM25/Hybrid Search Configuration
enable_hybrid_search = false           # Combine vector + BM25 search
vector_weight = 0.7                    # Weight for vector search (0.0-1.0)
bm25_k1 = 1.2                         # BM25 term frequency saturation
bm25_b = 0.75                         # BM25 field length normalization
bm25_min_token_length = 2             # Minimum token length for BM25
bm25_rebuild_threshold = 1000         # Rebuild BM25 index after N changes

# Query Enhancement Configuration
enable_query_enhancement = true        # Enable intelligent query enhancement
query_enhancement_synonyms = false    # Disabled - can harm precision in technical content
query_enhancement_spelling = true     # Correct common spelling errors
max_synonyms_per_word = 2             # Maximum synonyms to add per word
query_enhancement_cache_ttl_days = 30 # Cache enhanced queries for N days
spelling_correction_threshold = 0.8   # Minimum similarity for spell correction
synonym_relevance_threshold = 0.6     # Minimum relevance for synonym inclusion

# Reranking Configuration (improves accuracy by 20-40%)
enable_reranking = true               # Enable cross-encoder reranking (disable for low-latency needs)
reranking_model = cross-encoder/ms-marco-MiniLM-L-6-v2  # Reranking model
reranking_top_k = 20                  # Number of results to rerank
reranking_batch_size = 32             # Batch size for reranking
reranking_device = cpu                # Device for reranking (cpu/cuda)
reranking_cache_size = 1000           # Number of reranking scores to cache