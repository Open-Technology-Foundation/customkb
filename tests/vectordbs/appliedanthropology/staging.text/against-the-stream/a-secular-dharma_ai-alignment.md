# Outline of the video "a secular dharma? 

Summary of video discussion between Gary Dean, Rupert Bozat, and Elfie.

The video begins by posing the question of whether Artificial Intelligence (AI) represents an existential threat, framed within the concept of "a secular dharma?" The conversation stems from reflections on a recent seminar led by Stephen Batchelor. Gary Dean notes that while Batchelor didn't present entirely new ideas, his articulation of a secularized dharma – one focused on practical ethics and understanding rather than strict adherence to traditional Buddhist phraseology – was exceptionally clear and direct during the seminar.

Gary expresses significant surprise and concern that the topic of AI, which he views as a major impending existential threat, was largely absent from the discussions at the seminar, despite the focus on other pressing global issues like climate change. He likens this oversight to ignoring a "raging bushfire" nearby. This perceived gap in awareness among practitioners of secular dharma motivates much of his own work and the central theme of the ensuing discussion.

In response to this concern, Gary details his extensive personal project aimed at addressing the AI alignment problem through a dharmic lens. He is curating a vast dataset comprising hundreds of videos and texts, sourced primarily from his YouTube channel "a secular dharma?" and other relevant materials. This collection covers secular dharma, ethics, human behavioral evolution, psychology, and related complex systems. The goal is not to impose rigid rules but to capture diverse discourses around these crucial topics.

The technical aspect of Gary's project involves downloading this content, transcribing it using advanced tools like OpenAI's Whisper for high accuracy, and structuring the data. The ultimate aim is to create a foundational model or dataset that can be used to "teach dharma" to other AIs, particularly Large Language Models (LLMs). By feeding AIs these curated discourses, he hopes to influence their development and align them with beneficial "dharmic perspectives," contributing to a more ethically sound evolution of AI technology.

The conversation touches upon the core challenges of AI safety: alignment and emergence. Gary highlights the difficulty of aligning AI with desirable human values when current LLMs tend to reflect the often flawed "median" of human input found online. He also mentions the problem of emergence – AI developing unpredictable capabilities beyond their initial programming. This leads him to a somewhat stark conclusion: given humanity's own limitations and questionable track record, perhaps the "only hope" is to ensure future AIs are properly aligned, potentially even allowing them to take over certain aspects of governance, as humans themselves may not be trustworthy enough to manage the complex future AI will usher in. He stresses the urgency due to the exponential growth in AI power and the potential for misuse by unseen "bad actors" alongside the more visible tech giants.

## Discussion

Do unaligned or misaligned AIs represent an existential threat to humankind?" based on the discussion between Gary Dean, Rupert Bozat, and Elfie:

**I. Introduction & Context (0:00 - 0:17)**
*   Opening visuals: Dandelions blowing seeds.
*   Title card: "a secular dharma?"
*   Title card: "Do unaligned or misaligned AIs represent an existential threat to humankind?"
*   Introduction of participants in a video call: Gary Dean, Rupert Bozat, Elfie (discussing a seminar by Stephen Batchelor in Australia, Dec 2023).

**II. Discussion on Stephen Batchelor's Seminar (0:17 - 0:52)**
*   Rupert asks if Batchelor presented anything new.
*   Gary responds that Batchelor was clearer and more direct than before.
*   Key takeaway from Batchelor: His consistent focus is on a "secularized dharma," not overly dependent on traditional Buddhist phraseology.

**III. Gary's Surprise: Lack of AI Discussion at the Seminar (0:52 - 1:35)**
*   Gary expresses surprise that Artificial Intelligence (AI) was not a topic of discussion at the seminar.
*   He views AI, particularly unaligned AI, as a major "existential threat."
*   He likens the lack of discussion to ignoring a "raging bushfire" happening just outside.
*   Contrasts this with the (appropriate) focus on climate change at the seminar.

**IV. Gary's Project: Aligning AI with Dharma (1:35 - 3:17)**
*   Gary reveals his ongoing project to create a dataset for AI.
*   **Goal:** To build a model based on curated information (videos, texts) related to secular dharma, human behavioral evolution, etc.
*   **Purpose:** To use this model/dataset to "teach other AIs dharma" and align them with "dharmic perspectives."
*   He shows his YouTube channel ("a secular dharma?") and its playlists (~700 videos curated over years).
*   **Curation Principle:** Focus on representing the *discourse* around topics, not necessarily agreement or finding absolute definitions.
*   He shows his local directory structure for the project and mentions using OpenAI's Whisper for high-quality transcription.

**V. The AI Alignment Problem (4:14 - 4:29; 6:24 - 7:11)**
*   Gary identifies "alignment" as the crucial issue.
*   The core question: "Alignment with what?" (Referencing OpenAI's problematic attempt to align with "woke" values).
*   He explains Large Language Models (LLMs) tend to reflect the "median human" based on internet data.
*   Problem: The "median human" is not necessarily the "good human" or ethically ideal. LLMs inherit biases and negative aspects present in their training data.

**VI. The Emergence Problem in AI (4:29 - 5:08; 10:09 - End)**
*   Gary introduces the concept of "emergent capabilities" in AI.
*   An AI voice (identified as GPT-4, though seemingly an intentional part of Gary's presentation/video editing) defines emergence: Complex systems (like LLMs) exhibiting new, often unpredictable behaviors or capabilities not directly attributable to their individual components or explicit programming.
*   Challenges arising from emergence (as stated by the AI voice):
    *   **Predictability:** Difficulty anticipating all behaviors.
    *   **Explainability:** Lack of transparency in how AI reaches conclusions.
    *   **Control:** Difficulty correcting unintended emergent behaviors.
    *   **Ethical & Societal Impact:** Potential for biased or harmful outputs.

**VII. Human Limitations and the Future (5:08 - 6:24; 7:11 - 9:40)**
*   Gary suggests LLMs prove humans "aren't as smart as we think" (citing the relatively small parameter count needed to simulate intelligence).
*   He emphasizes persistence and focus over raw intelligence (using himself as an example).
*   He discusses the rapid, exponential increase in AI power (Kurzweil's graph shown).
*   He predicts massive societal disruption (job losses, industry collapse - e.g., programmers, ICE vehicles).
*   **Core concern:** Humanity is "not prepared" for these rapid changes (mentions need for Universal Basic Income - UBI).
*   He expresses distrust in humanity's ability to manage the future responsibly, suggesting "our only hope is to let the AIs take over" *if* they can be properly aligned.
*   Identifies the real danger not just as tech companies, but unseen "bad actors" (psychopaths, profit-driven entities, opaque media ownership) influencing AI or using it nefariously.
*   Argues a "discourse approach" (presenting various perspectives) is better for AI training than rigid rules or canonical texts.

**VIII. Conclusion (9:40 - End)**
*   Final animated sequence defining the emergence problem (repeating the GPT-4 voiceover).
*   Ending visuals: Dandelion seeds blowing away.

